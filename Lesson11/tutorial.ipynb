{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links for more Information and Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.slideshare.net/SarahGuido/kmeans-clustering-with-scikitlearn<BR>\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#example-cluster-plot-kmeans-digits-py<BR>\n",
    "http://scikit-learn.org/stable/auto_examples/document_clustering.html#example-document-clustering-py<BR>\n",
    "https://www.mail-archive.com/scikit-learn-general@lists.sourceforge.net/msg08695.html<BR>\n",
    "http://en.wikipedia.org/wiki/Cluster_analysis<BR>\n",
    "http://docs.scipy.org/doc/scipy-0.14.0/reference/cluster.html<BR>\n",
    "http://scikit-learn.org/stable/modules/clustering.html<BR>\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster<BR>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering<BR>\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#example-cluster-plot-agglomerative-clustering-metrics-py<BR>\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#example-cluster-plot-digits-linkage-py<BR>\n",
    "http://people.cs.uchicago.edu/~dinoj/manifold/swissroll.html<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the basic abilities that humans have excercised since primitive times is to divide the known world into separate classes where individual objects share common features. Starting with primitive cave dwellers classifying the natural world they lived in, distinguishing plants and animals useful or dangerous for their survival, we arrive at modern times in which marketing departments classify consumders into target segments and then act with proper marketing plans.</br>\n",
    "\n",
    "A data-driven approach to classification, called clustering, will prove to be of great help in achieving success for your data project when you need to provide new insights from scratch. </br>\n",
    "\n",
    "Clustering techniques are a set of unsupervised classifiction methods that can create meaningful classes by directly processing your data, without any previous knowledge or hypothesis about the group that may be present. If all supervised algorithms need label examples (class labels), unsupervised ones can figure out by themselves what the most appropriate labels could be.\n",
    "\n",
    "Most of the time we use partition-clustering techniques (a data point can be part of only one group, so the groups dont overlap; their memebrship is distinct) and among partitioning methods, we use K-means the most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is an iterative algorithm that has become very popular in machine learning because of its simplicity, speed and scalability to a large number of data points. The K-means algorithm relies on the idea that there are a specific number of data groups, called clusters. Each data group is scattered around a central point which they share some key characteristics.</br>\n",
    "\n",
    "You can actually imagine the central point of a cluster, called a centroid, as a sun. The data points distribute around the centroid like planets. Clusters are also expected to clearly separate from each other.</br>\n",
    "\n",
    "Given such assumptions, all you have to do is to specifiy the number of groups you expect (you can use a guess or try a number of possible desirable solutions), and the K-means algorithm will look for them, using a heuristic in order to recover the position of the central points.</br>\n",
    "\n",
    "The procedure for finding the centroids is starightforward: </br>\n",
    "\n",
    "- Guess a K number of clusters. </br>\n",
    "- Form the initial clusters.  </br>\n",
    "- Reiterate the clusters until you notice that your solution does not change anymore. </br>\n",
    "\n",
    "You recalculate the centroids as an average of all the points present in the group. All the data points are reassigned to the groups based on the distance from the new centroids. The iterative process of assigning cases to the most plausible centroid and then averaging the assigned ones to find a new centroid will slowly shift the centroid position toward the areas where most data points gravitate. The result is that you end up with the true centroid position. </br>\n",
    "\n",
    "The procedure has only two weak points: </br>\n",
    "\n",
    "- You choose the initial centroids randomly, which means that you could start from a bad starting point. As a result, the iterative process will stop at some unlikey solution. To ensure that your solution is the most probable, you have to try the algorithm a few times and track the results. The more often you try the more likely you are to confirm the right solution. The Python Scikit-learn implementation of K-means will do that for you.\n",
    "\n",
    "- The K-means uses the Euclidean distance, which is the distance between two points on a plane. In a K-means application, each data point is a vector of features, so when comparing the distance of two points, you do the following:\n",
    "\n",
    "    a) Create a list containing the differences of the elements in the two vectors. <br/>\n",
    "    b) Square all the elements of the difference vector. <br/>\n",
    "    c) Calculate the square root of the summed elements. <br/>\n",
    "\n",
    "\n",
    "** Please refer to below section for Euclidean distance easy tutorial first. <br />\n",
    "\n",
    "In the end, the Euclidean is really just a big sum. When the variables making up the difference vector are significantly different in scale from each other, you end up with a distance dominated by the elements with the largest scale. It is very important to rescale the variables so that they use a similar scale before applying the K-means algorithm. You can use fixed range or a statistical normalization with zero mean and unit variance to achieve this goal. <br/>\n",
    "\n",
    "Another problem that may arise, apart from scale, is due to correlation between variables, causing redundancy of information. If two variables are highly correlated, that means that a part of their information content is repeated. Replication implies counting the same information more than once in the summation used to calculate the distance. If you are not aware of the correlation issue, some variables will dominate your distance measure calculation; a situation that may lead to not finding the useful clusters that you want. The solution is to remove the correlation thanks to a dimensionality reduction algorithm such as Principle Component Analysis (PCA). Scikit-learn has a function in the preprocessing module that can correctly scale your variable, as well as a function for PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean distance easy tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([165, 55, 70])\n",
    "B = np.array([185, 60, 30])\n",
    "\n",
    "D = (A-B)\n",
    "D = D**2\n",
    "D = np.sqrt(np.sum(D))\n",
    "\n",
    "print D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ideal example is clustering the handwritten digits dataset provided by Scikit-learn package. We want to cluster handwritten numbers. The numbers data are in form of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print digits.DESCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 x 8 pixels for each digit and value of each element is between 0 - 15\n",
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is all the digits (8 x 8) and ground_truth is the their real label (what digit they are)\n",
    "X = digits.data\n",
    "ground_truth = digits.target\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "# this means they are 64 (8 x 8) attributes, just use 40 \n",
    "pca = PCA(n_components=40)\n",
    "# scale the X\n",
    "Cx = pca.fit_transform(scale(X))\n",
    "# the following line calculates what percentage of original variation in data was retained even though using 40 \n",
    "# components\n",
    "print 'Explained variance %0.3f' % sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# use 10 centroids, iterate 10 times\n",
    "# random_state = The generator used to initialize the centers. If an integer is given, it fixes the seed. \n",
    "clustering = KMeans(n_clusters=10, n_init=10, random_state=1)\n",
    "clustering.fit(Cx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels are all clustered from 0 - 9. This 0 - 9 means they are bundled in these labels. 0 -9 does not mean the digits 0 - 9\n",
    "# that we have the ground_truth\n",
    "len(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# makes them two columns\n",
    "ms = np.column_stack((ground_truth,clustering.labels_))\n",
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ms, columns = ['Ground truth','Clusters'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creates a cross tabular form\n",
    "pd.crosstab(df['Ground truth'], df['Clusters'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how numbers such as seven or zero are concentarted into their own cluster, but others such as 3 and 9 tend to gather together into the same group, the cluster 1. (Note: rows are digits but columns does not mean the same digit it means they are bundled in a cluster names 0 or 1 or ...) <br/>\n",
    "\n",
    "This means 0 or 7 easily identifiable but 3 and 9 gets confused with eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How good is the results?\n",
    "from sklearn import metrics\n",
    "metrics.adjusted_rand_score(ground_truth,clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform the cluster to cluster-distance space\n",
    "# this is not that important, is just extra info\n",
    "dist = clustering.fit_transform(Cx)\n",
    "print len(dist)\n",
    "# retrun the minimums on rows\n",
    "print np.min(dist,axis=0)\n",
    "# return the inidices of the mins along rows\n",
    "print np.argmin(dist,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use inertia to measure the viability of a cluster. Inertia is the sum of all the differences between every cluster member and its centroid. If the examples in the group are similar to the centroid, the difference is small and so is the inertia. Inertia as an individual measure reveals little. What you want to do instead of using inertia directly is to compare the inertia of a cluster solution with the previous cluster solution. This comparison provides you with the rate of change, a more interpretable measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inertia = list()\n",
    "delta_inertia = list()\n",
    "for k in range(1,21):\n",
    "    clustering = KMeans(n_clusters=k, n_init=10, random_state=1)\n",
    "    clustering.fit(Cx)\n",
    "    if inertia: # makes sure you wont do if block logic if list is empty\n",
    "        delta_inertia.append(inertia[-1] - clustering.inertia_)\n",
    "    inertia.append(clustering.inertia_)\n",
    "    \n",
    "delta_inertia    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([k for k in range(1,21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.figure()\n",
    "plt.plot([k for k in range(2,21)], delta_inertia, 'ko-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Rate of change of inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate of change in inertia will provide you with just a few tips where there could be good cluster solutions. It is up to you to decide which to pick if you need to get some extra insight on data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
