{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing the number: linear regression\n",
    "\n",
    "Regression has a long history in statistics, from building simple but effective linear models of economic, psychological, social and political data. <br />\n",
    "\n",
    "Regression is also a common tool in data science. Data science practitioners see linear regression as a simple, understandable, yet effective algorithm for estimations, and, in its logistic regression version, for classification as well. <br />\n",
    "\n",
    "Linear regression is a statistical model that defines the relationship between a target variable and a set of predictive features. It does so using a formula: <br />\n",
    "\n",
    "y = a + bx <br />\n",
    "\n",
    "You can translate this formula into something readable and useful for many problems. For instance, if you are trying to guess your sales based on historical results and available data about advertising expenditures, the same preceding formula becomes <br />\n",
    "\n",
    "sales = a + b * (advertising expenditure) <br />\n",
    "\n",
    "You may already have encountered this formula during high school because its also the formula of a line in a bidimensional plane, which is made of an x axis and a y axis. <br />\n",
    "\n",
    "If b is positive, y increases and decreases as x increases and decreases -- when b is negative, y behaves in the opposite manner. When the value of b is near zero, the effect of x on y is slight, but if the value of b is high, either positive or negative, the effect of changes in x on y are great. <br />\n",
    "\n",
    "You can express this relationship graphically as the sum of the square of all the vertical distances between all the data points and the regression line. Such a sum is always the minimum possible when you calculate the regression line correctly using an estimation called ordinary least squares. The difference between the real y values and the regression line (the predicted y values) are defined as residuals. (errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using more variables\n",
    "\n",
    "When using a single variable for predicting y, you use simple linear regression, but when working with many variables, you use multiple linear regression. When you have many variables, their scale isnt important in creating precise linear regression predictions. But as a good habit is to standardize X because the scale of the variables is quite important for some variants of regression. <br />\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in the scikit: they might behave badly if the individual feature do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. <br />\n",
    "\n",
    "The following example relies on the Boston dataset from Scikit-learn. It tries to guess Boston housing prices using a linear regression. The example also tries to determine which variables influence the result more. So the example standardizes the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "boston = load_boston()\n",
    "# the last attribute (MEDV) is the target\n",
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = scale(boston.data), boston.target\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# if normalize True it normalizes the X\n",
    "regression = LinearRegression(normalize=True)\n",
    "regression.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the algorithm is fitted, you can use the score method to report the R2 measure, which is a measure that ranges from 0 to 1 and points out how using a particular regression model is better in predicting y than using a simple mean would be. You can also see R2 as being the quantity of target information explained by the model, so getting near 1 means being able to explain most of the y variable using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print regression.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating R2 on the same set of data used for the training is common in statistics. In data science and machine-learning, its always better to test scores on data that has not been used for training. Algorithms of greater complexity can memorize the data better than they learn from it, but this statement can be also true sometimes for simpler models, such as linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what drives the estimates in the multiple regression model, you have to look at the coefficients_ attribute, which is an array containing the regression beta (b) coefficients. Printing at the same time, the boston.DESCR attribute helps you understand which variable the coefficients reference. The zip function will generate an iterable of both attributes, and you can print it for reporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [a+':'+str(round(b,1)) for a, b in zip(boston.feature_names, regression.coef_,)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see DIS has the most significant influence. DIS is the weighted distances to five employment centers. In real state, a house that is too far from people's interests (such as work) lowers the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding limitations and potential problems\n",
    "\n",
    "\n",
    "Although linear regression is simple yet effective estimation tool, it has quite a few problems. The problems can reduce the benefit of using linear regressions in some cases, but it really depends on the data. <br />\n",
    "\n",
    "- Linear regression can model only quantitative data. When modeling categories as response, you need to modify the data into a logistic regression.\n",
    "- If data is missing and you dont deal with it properly, the model stops working.\n",
    "- Outliers are quite disruptive for a linear regression because linear regression tries to minimize the square value of the residuals, and outliers have big residuals.\n",
    "- The relation between the target and each particular variable is based on single coefficient - there isnt automatic way to represnt complex relations like parabola (there is unique value of x maximizing y) or exponential growth.\n",
    "- The greatest limitation is that linear regression provides a summation of terms, which can vary independently of each other. It is hard to figure out how to represent the effect of certain variables that affect the result in very different ways according to their value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Logistic Regression\n",
    "\n",
    "Linear regression is well suited for estimating values, but it isnt the best tool for predicting the class of an observation. In spite of the statistical that advises against it, you can try to clasify a binary class by forcing one class as 1 and the other as 0. The results are disappointing most of teh time, so the statistical theory wasnt wrong! <br /><br />\n",
    "The fact is that linear regression works on a continuum of numeric estimates. In order to classify correctly, you need a more suitable measure, such as the probability of class ownership. Thanks to following formula, you can transform a linear regression numeric estimate into a probability that is more apt to describe how a class fits on observation:   <br /><br />\n",
    "probability of a class = exp(r) / (1 + exp(r)) <br /><br />\n",
    "r is the regression result (a + bx) and we maximize the probability of a class based on the data we have. A linear regression using such a formula for transforming its results into probabilities is a logistic regression.<br />\n",
    "\n",
    "https://datajobs.com/data-science-repo/Logistic-Regression-%5BPeng-et-al%5D.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "# these are characteristic data for each iris\n",
    "print iris.data\n",
    "# these are each iris type that above characteristic belongs to\n",
    "print iris.target\n",
    "# X is all the rows of data but the last one which used for testing\n",
    "# y is all targets but the last one which is used for testing\n",
    "X, y = iris.data[:-1,:], iris.target[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X,y)\n",
    "# testing the model on the last data and target we kept for testing\n",
    "print 'Predicted class %s, real class %s' % (logistic.predict(iris.data[-1,:]),iris.target[-1])\n",
    "print 'Probabilities for each class from 0 to 2: %s' % logistic.predict_proba(iris.data[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using probabilities let you guess the most probable class, but you can also order the predictions with respect to being part of that class. This is especifically useful for medical purposes. Ranking a prediction in terms of likelihood with respect to others can reveal what patients are at most risk of getting or already having a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering when classes are more than two\n",
    "\n",
    "Most algorithms provided by Scikit-learn that predict probabilities or a score for class can automatically handle multiclass problems using two different strategies: <br/> \n",
    "\n",
    "<b>One versus rest:</b> The algorithm creates one model for each class<br /><br />\n",
    "<b>One versus one:</b> The algorithm compares every class against every individual remaining class, building a number of models equivalent to n* (n - 1) /2, where n is the number of classes.<br />\n",
    "\n",
    "In the case of logistic regression, the default multiclass strategy is the one versus the rest.\n",
    "\n",
    "For following example: http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html\n",
    "\n",
    "There are hand-written numbers each on 8 by 8 pixels. So the dataset is made up of 1797 8x8 images. Each pixel based on the coloring from complete white to complete black can go from 0 to 16. We want to predict new numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "# each row has 64 columns (8 * 8) and 1797 rows\n",
    "print digits.data\n",
    "# the target is numbers from 0 to 9\n",
    "print digits.target\n",
    "# using 1700 rows for as training set\n",
    "X, y = digits.data[:1700,:], digits.target[:1700]\n",
    "# using 97 rows for testing\n",
    "tX, ty = digits.data[1700:,:], digits.target[1700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "logistic = LogisticRegression()\n",
    "OVR = OneVsRestClassifier(logistic).fit(X,y)\n",
    "OVO = OneVsOneClassifier(logistic).fit(X,y)\n",
    "print 'One vs rest accuracy: %.3f' % OVR.score(tX,ty)\n",
    "print 'One vs one accuracy: %.3f' % OVO.score(tX,ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X,y)\n",
    "print 'One vs rest accuracy: %.3f' % LR.score(tX,ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the one-versus-one strategy obtained the best accuracy thanks to its high number of models in competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Things as Simple as Na√Øve Bayes\n",
    "\n",
    "<br /><b>First, Conditional Probability & Bayes' Rule</b><br /><br />\n",
    "Before someone can understand and appreciate the nuances of Naive Bayes', they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes' Rule. \n",
    "\n",
    "Conditional Probability in plain English: What is the probability that something will happen, given that something else has already happened.\n",
    "\n",
    "Let's say that there is some Outcome O. And some Evidence E. From the way these probabilities are defined: The Probability of having both the Outcome O and Evidence E is: (Probability of O occurring) multiplied by the (Prob of E given that O happened)\n",
    "\n",
    "One Example to understand Conditional Probability:\n",
    "\n",
    "Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female.\n",
    "\n",
    "If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that.\n",
    "\n",
    "Probability of (Democrat and Female Senator)= Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat.\n",
    "\n",
    "  P(Democrat & Female) = P(Democrat) x P(Female / Democrat) \n",
    "\n",
    "We could compute the exact same thing, the reverse way:\n",
    "\n",
    "  P(Democrat & Female) = P(Female) x P(Democrat / Female) \n",
    "\n",
    "\n",
    "<br /><b>Understanding Bayes Rule</b><br /><br />\n",
    "\n",
    "Conceptually, this is a way to go from P(Evidence/ Known Outcome) to P(Outcome/Known Evidence). Often, we know how frequently some particular evidence is observed, given a known outcome. We have to use this known fact to compute the reverse, to compute the chance of that outcome happening, given the evidence.\n",
    "\n",
    "P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence)\n",
    "\n",
    "The classic example to understand Bayes' Rule:\n",
    "\n",
    "Probability of Disease D given Test-positive = \n",
    "\n",
    "     Prob(Test is positive/Disease) *P(Disease) \n",
    "     _______________________________________________________\n",
    "     (scaled by) Prob(Testing Positive, with or without the disease)\n",
    "     \n",
    "     \n",
    "or simply:\n",
    "\n",
    "     P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "Now, all this was just preamble, to get to Naive Bayes.\n",
    "\t\n",
    "<br /><b>Getting to Naive Bayes'</b><br /><br />\n",
    "\n",
    "So far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given multiple evidence. In that case, the math gets very complicated. To get around that complication, one approach is to 'uncouple' multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called naive Bayes.\n",
    "\n",
    "P(Outcome/Multiple Evidence) = \n",
    "P(Evidence1/Outcome) x P(Evidence2/outcome) x ... x P(EvidenceN/outcome) x P(Outcome)\n",
    "scaled by P(Multiple Evidence)\n",
    "\n",
    "Many people choose to remember this as:\n",
    "\n",
    "P(outcome/evidence) = P(Likelihood of Evidence) x Prior prob of outcome\n",
    "                      ___________________________________________\n",
    "                           P(Evidence)\n",
    "\n",
    "Notice a few things about this equation:\n",
    "\n",
    "    If the Prob(evidence/outcome) is 1, then we are just multiplying by 1.\n",
    "    If the Prob(some particular evidence/outcome) is 0, then the whole prob. becomes 0. If you see contradicting evidence, we can rule out that outcome.\n",
    "    Since we divide everything by P(Evidence), we can even get away without calculating it.\n",
    "    The intuition behind multiplying by the prior is so that we give high probability to more common outcomes, and low probabilities to unlikely outcomes. These are also called base rates and they are a way to scale our predicted probabilities.\n",
    "\n",
    "<br /><b>How to Apply NaiveBayes to Predict an Outcome?</b><br /><br />\n",
    "\n",
    "Just run the formula above for each possible outcome. Since we are trying to classify, each outcome is called a class and it has a class label. Our job is to look at the evidence, to consider how likely it is to be this class or that class, and assign a label to each entity. Again, we take a very simple approach: The class that has the highest probability is declared the \"winner\" and that class label gets assigned to that combination of evidences.\n",
    "Fruit Example\n",
    "\n",
    "Let's try it out on an example to increase our understanding: The OP asked for a 'fruit' identification example.\n",
    "\n",
    "Let's say that we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit:\n",
    "\n",
    "    Whether it is Long\n",
    "    Whether it is Sweet and\n",
    "    If its color is Yellow.\n",
    "\n",
    "This is our 'training set.' We will use this to predict the type of any new fruit we encounter.\n",
    "\n",
    "Type           Long | Not Long || Sweet | Not Sweet || Yellow |Not Yellow|Total  <br />\n",
    "             ___________________________________________________________________ <br />\n",
    "Banana      |  400  |    100   || 350   |    150    ||  450   |  50      |  500  <br />\n",
    "Orange      |    0  |    300   || 150   |    150    ||  300   |   0      |  300  <br />\n",
    "Other Fruit |  100  |    100   || 150   |     50    ||   50   | 150      |  200  <br />\n",
    "            ____________________________________________________________________ <br />\n",
    "Total       |  500  |    500   || 650   |    350    ||  800   | 200      | 1000  <br />\n",
    "             ___________________________________________________________________ <br />\n",
    "\n",
    "We can pre-compute a lot of things about our fruit collection.\n",
    "\n",
    "The so-called \"Prior\" probabilities. (If we didn't know any of the fruit attributes, this would be our guess.) These are our base rates.\n",
    "\n",
    " P(Banana)  = 0.5 (500/1000)\n",
    " P(Orange)  = 0.3\n",
    " P(Other Fruit) = 0.2\n",
    "\n",
    "Probability of \"Evidence\"\n",
    "\n",
    "p(Long)  = 0.5\n",
    "P(Sweet)  = 0.65\n",
    "P(Yellow) = 0.8\n",
    "\n",
    "Probability of \"Likelihood\"\n",
    "\n",
    "P(Long/Banana) = 0.8\n",
    "P(Long/Orange) = 0  [Oranges are never long in all the fruit we have seen.]\n",
    " ....\n",
    "\n",
    "P(Yellow/Other Fruit) =  50/200 = 0.25\n",
    "P(Not Yellow/Other Fruit)  = 0.75\n",
    "\n",
    "<br /><b>Given a Fruit, how to classify it?</b><br /><br />\n",
    "\n",
    "Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?\n",
    "\n",
    "We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set):\n",
    "\n",
    "P(Banana/Long, Sweet and Yellow) = P(Long/Banana) P(Sweet/Banana) P(Yellow/Banana) x P(banana)\n",
    "                                            __________________________________________________\n",
    "                                                   P(Long). P(Sweet). P(Yellow) \n",
    "\n",
    "                                   0.8 x 0.7 x 0.9 x 0.5\n",
    "                              =    ______________________ \n",
    "                                     P(evidence)\n",
    "\n",
    "                          = 0.252/P(evidence)\n",
    "\n",
    "P(Orange/Long, Sweet and Yellow) = 0\n",
    "\n",
    "P(Other Fruit/Long, Sweet and Yellow) = P(Long/Other fruit) x P(Sweet/Other fruit) x P(Yellow/Other fruit) x P(Other Fruit)\n",
    "                                     = (100/200 x 150/200 x 50/150 x 200/1000) / P(evidence)\n",
    "                                     = 0.01875/P(evidence)\n",
    "\n",
    "By an overwhelming margin (0.252 >> 0.01875), we classify this Sweet/Long/Yellow fruit as likely to be a Banana.\n",
    "\n",
    "\n",
    "<br /><b>Why it is called Naive bayes?</b><br /><br />\n",
    "\n",
    "If you look at example above on the right hand side of the equiation:\n",
    "\n",
    "P(Long, Sweet, Yellow / Banana) x P(banana) -->  P(Long/Banana) P(Sweet/Banana) P(Yellow/Banana) x P(banana)\n",
    "\n",
    "This means Naive bayes assumption is that those porbabilities are independent from each other. This is not the case always but this simplification make Naive bayes practical tool.\n",
    "\n",
    "It is ok to use everything for prediction, even though it seems as though it shouldnt be okay given the strong association between variables. Here are some of the ways in which you commonly see Naive Bayes used: <br />\n",
    "\n",
    "- Building Spam detectors\n",
    "- Sentiment analysis (guessing wheather a text contains positive or negative attitutdes with respect to topic, and detecting the mood of the speaker)\n",
    "- Text-processing tasks such as spell correction\n",
    "\n",
    "Naive Bayes is also popular because it does not need as much data to work. It can naturally handle multiple classes. With some slight variable modifications, it can also handle numeric variables. Scikit-learn provides three Naive Bayes classes in the sklearn.naive_bayes module:\n",
    "\n",
    "- MultinominalNB: Uses the probabilities derived from a feature'spresence. \n",
    "- BernolliNB: Provides the multinominal functionality of Naive Bayes but it penalizes the absence of a feature.\n",
    "- GaussianNB: Defines a version of Naive Bayes that expects a normal distribution of all the features. If your variables have positive and negative values, this is the best choice.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Hashing Trick\n",
    "\n",
    "Scikit-learn provides you with most of the data structures and functionality you need to complete your data science project. There are even classes for the trickiest and most advanced problems.\n",
    "\n",
    "For instance, when dealing with text, one of the most useful solutions provided by the Scikit-learn package is the hashing trick. We talked about how to work with text by using the bag of words model. All these powerful transformations can operate properly only if all your text is known and available in the memory of your computer.\n",
    "\n",
    "A more serious data science challenge is to analyze online-generated text flows, such as from social netwroks or large online text repositories. Hashing trick can give you quite a few advantages here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hash functions\n",
    "\n",
    "Hash functions can transform any input into an output whose characteristics are predictable. The most useful hash function characteristic is that, given a certain input, they always provide the same numeric output value. Consequently, they are called deterministic functions. For example, input a word like dog and teh hashing function always returns the same number.\n",
    "\n",
    "In a certain sense, hash functions are like secret code, transformin every thing into numbers. Unlike secret codes, however, you cannot convert the hashed code to its original value. In addition, in some rare cases, diffrent words generate the same hashed result (also called a hash collision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the hashing trick\n",
    "\n",
    "There are many hash functions, with MD5 (often used to check file integrity, because you can hash the entire files) and SHA (used in cryptography) being the most popular. Python posseses a built-in hash function named hash. You can test Python hash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash('Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python session on your computer may return a different value than the one shown on the preceding line. When you need consistent output, rely on the Scikit-learn hash functions instead because the output is consistent across machines.\n",
    "\n",
    "A Scikit-learn hash function can also return an index in a specific positive range. You can obtain something similar using a built-in hash by employing standard division and its remainder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abs(hash('Python')) % 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, pretend that you want to transform a text string from the internet into a numeric vector (a feature vector) so that you can use it for starting a machine learning project. A good strategy for managing this data science task is to employ one-hot-encoding, which produces a bag of words. Here are the steps for one-hot-encoding a string(\"Python for data science\") into a vector.\n",
    "\n",
    "- Assign a number to each word, for instance, Python=0 for=1 data=2 science=3.\n",
    "- Initialize the vector, counting the number of unique words that you assigned a code in Step1.\n",
    "- Use the codes assigned in Step1 as indices for populating the vector with values, assigning a 1 where there is a coincidence with a word existing in the phrase.\n",
    "\n",
    "The resulting feature vector is expressed as the sequence [1,1,1,1] and made of exactly four elements. When suddenly a new phrase arrives and you must vectorize the following text as well: \"Python for machine learning\". Now you have two new words \"machine learning\" to work with. The following steps help you create the new vectors:\n",
    "\n",
    "- Assign these new codes: machine=4 learning=5\n",
    "- Enlarge the previous vector to include the new words: [1,1,1,1,0,0]\n",
    "- Compute the vector for the new string: [1,1,0,0,1,1]\n",
    "\n",
    "One-hot-encoding is quite optimal because it creates efficient and ordered feature vectors. Unfortunately, one-hot-encoding fails and becomes difficult to handle when your project experiences a lot of variablity with regards to its inputs. This is a common situation in data science projects working with text or other symbolic features where flow from the Internet or other online environments can suddenly craete or add to your initial data. Using hash functions is a smarter way to handle unpredictability in your inputs.\n",
    "\n",
    "In Python, you can define a simple hashing trick by creating a function and checking the results using the two test strings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_1 = 'Python for data science'\n",
    "string_2 = 'Python for machine learning'\n",
    "\n",
    "def hashing_trick(input_string, vector_size=20):\n",
    "    feature_vector = [0] * vector_size\n",
    "    for word in input_string.split(' '):\n",
    "        index = abs(hash(word)) % vector_size\n",
    "        feature_vector[index] = 1\n",
    "    return feature_vector\n",
    "\n",
    "print hashing_trick(input_string='Python for data science', vector_size=20)\n",
    "print hashing_trick(input_string='Python for machine learning', vector_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing feature vectors, you should notice that:\n",
    "\n",
    "- You dont know where each word is located. When its important to be able to reverse the process of assigning words to indices, you must store the relationship between words and their hashed values seperately.\n",
    "- For small values of the vector_size function parameter, many words overlap in the same positions in the least representing the feature vector. (Use greater vector_size to minimize the overlap)\n",
    "\n",
    "The feature vectors in this example are made mostly of zero entries, representing a waste of memory when compared to the more memory-efficient one-hot-encoding. One of the ways in which you can solve this problem is to rely on sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with deterministic selection\n",
    "\n",
    "Sparse matrices are the answer when dealing with data that has few values, that is, when most of the matrix values are zeros. Sparse matrices store just the coordinates of the cells and their values. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "print csc_matrix([1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist, you dont have to worry about programming your own version of the hashing unless you would like some customization. Scikit-learn offers HashingVectorizer, a class that rapidly transforms any collection of text into a sparse data matrix using hashing trick. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "one_hot_encoder = CountVectorizer()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(['Python for data science','Python for machine learning'])\n",
    "# two rows for each memeber of list and total of 6 different words for each row\n",
    "# it uses compressed sparse row so total of 8 stored values\n",
    "one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print one_hot_encoder.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as new text arrives, CountVectorizer stops working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot_encoded.transform(['New text has arrived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HashingVectorizer, there is always a place for new words in the data matrix. At worst, a word settles in an already occupied position, causing a word collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "sklearn_hashing_trick = HashingVectorizer(n_features=20, binary=True, norm=None)\n",
    "hashed_text = sklearn_hashing_trick.transform(['Python for data science','Python for machine learning'])\n",
    "hashed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn_hashing_trick.transform(['New text has arrived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingVectorizer is the perfect function to use when your data cannot fit into memory and its features are not fixed. In the other cases, consider using the more intuitive CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting text classifications using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "# one of the posts in training data\n",
    "print newsgroups_train.data[0]\n",
    "print \"\\n\"\n",
    "# print all target names for this training data \n",
    "print newsgroups_train.target_names\n",
    "print \"\\n\"\n",
    "# print all the targets (categories of text) associated to each post. It corresponds to target_names\n",
    "print newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'number of posts in training: %i' % len(newsgroups_train.data)\n",
    "D={word:True for post in newsgroups_train.data for word in post.split(' ')}\n",
    "print 'number of distinct words in training: %i' % len(D)\n",
    "print 'number of posts in test: %i' % len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We set alpha values, which are useful for avoiding a zero probability for rare features (a zero probability would \n",
    "# exclude these features from the analysis)\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "Bernoulli = BernoulliNB(alpha=0.01)\n",
    "Multinomial = MultinomialNB(alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use two different hashing tricks, one counting the words (for the multinominal approach) and one recording wheather a word appeared in a binary variable (the binominal approach, all non zero counts are set to 1). You can also remove stop words, that is, common words found in the English language such as \"a,\" \"the,\" \"in,\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "multinomial_hashing_trick = HashingVectorizer(stop_words='english', binary=False, norm=None, non_negative=True)\n",
    "binary_hashing_trick = HashingVectorizer(stop_words='english', binary=True, norm=None, non_negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Multinomial.fit(multinomial_hashing_trick.transform(newsgroups_train.data),newsgroups_train.target)\n",
    "Bernoulli.fit(binary_hashing_trick.transform(newsgroups_train.data),newsgroups_train.target)\n",
    "from sklearn.metrics import accuracy_score\n",
    "for m,h in [(Bernoulli,binary_hashing_trick), (Multinomial,multinomial_hashing_trick)]:\n",
    "    print 'Accuracy for %s: %.3f' % (m, accuracy_score(y_true=newsgroups_test.target, y_pred=m.predict(h.transform(newsgroups_test.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Lazy Learning with K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting after observing neighbors\n",
    "\n",
    "\n",
    "lazy because least amount of work.\n",
    "Keep all records and their labels as traning when a new record comes find the k nearest records to those (euclidean distance) and deduct the similar label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "digits = load_digits()\n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(digits.data[:1700,:])\n",
    "X, y = pca.transform(digits.data[:1700,:]), digits.target[:1700]\n",
    "tX, ty = pca.transform(digits.data[1700:,:]), digits.target[1700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n",
    "kNN = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "kNN.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Accuracy: %.3f' % kNN.score(tX,ty) \n",
    "print 'Prediction: %s actual: %s' % (kNN.predict(tX[:10,:]),ty[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing wisely your k parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in [1,5,10,20,50,100,200]:\n",
    "    kNN = KNeighborsClassifier(n_neighbors=k).fit(X,y)\n",
    "    print 'for k=%3i accuracy is %.3f' % (k, kNN.score(tX,ty))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
