{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Working with HTML Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML and HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply extracting data from an XML file as you did before may not be enough. Following example extract XML data in correcttype so you be able perform data manipulation better on this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language.\n",
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "# Distutils to make Python modules and extensions, easily available to a wider audience with very little \n",
    "# overhead for build/release/install mechanics.\n",
    "# you can create setup.py etc for distribution\n",
    "# https://docs.python.org/2/library/distutils.html\n",
    "from distutils import util\n",
    "\n",
    "xml = objectify.parse(open('XMLData.xml'))\n",
    "root = xml.getroot()\n",
    "df = pd.DataFrame(columns=('Number', 'Boolean'))\n",
    "\n",
    "# it already knows we have 4 children\n",
    "for i in range(0,4):\n",
    "    obj = root.getchildren()[i].getchildren()\n",
    "    # make dics like {'Boolean': False, 'Number': 4}\n",
    "    # all data classes provide a .pyval attribute that returns the value as plain Python type\n",
    "    row = dict(zip(['Number', 'Boolean'], \n",
    "                   [obj[0].pyval, \n",
    "                    bool(util.strtobool(obj[2].text))]))\n",
    "    \n",
    "    \n",
    "    row_s = pd.Series(row)\n",
    "    row_s.name = obj[1].text\n",
    "    df = df.append(row_s)\n",
    "  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printing the type of first row and the Number and Boolean columns\n",
    "print type(df.ix['First']['Number'])\n",
    "print type(df.ix['First']['Boolean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XPath for data extraction\n",
    "\n",
    "http://www.w3schools.com/xsl/xpath_intro.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "import pandas as pd\n",
    "from distutils import util\n",
    "\n",
    "xml = objectify.parse(open('XMLData.xml'))\n",
    "root = xml.getroot()\n",
    "\n",
    "# map is Python built in function which applies function (1st arg) to every item of iterable (2nd arg)and \n",
    "# return a list of the results\n",
    "data = zip(map(int, root.xpath('Record/Number')), \n",
    "           map(bool, map(util.strtobool, \n",
    "                map(str, root.xpath('Record/Boolean')))))\n",
    "\n",
    "df = pd.DataFrame(data, \n",
    "                  columns=('Number', 'Boolean'), \n",
    "                  index=map(str, \n",
    "                        root.xpath('Record/String')))\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(df.ix['First']['Number'])\n",
    "print type(df.ix['First']['Boolean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and removing stop words\n",
    "\n",
    "Stemming is the process of reducing words to their stem (or root) word. For example, the words cats, catty, and catlike all have the stem cat. The act of stemming helps you analyze sentences by tokenizing them. Removing suffixes to create stem words and generally tokenizing sentences are only two parts of the process of creating something like a natural language interface. Languages include a great number of glue words that dont mean much to computer but have significant meaning to humans, such as a, as, the, that, and so on in English. These short, less useful words are stop words. Sentences dont make sense without them to humans, but for your computer, they can act as a means of stopping sentence analysis. <br /><br />\n",
    "\n",
    "This example requires the use of the Natural Language Toolkit (NLTK), which Anaconda does not install by default. So go here for instruction how to do it: http://www.nltk.org/data.html <br />\n",
    "\n",
    "After you install NLTK, you must also install the packages associated with it. The instructions at http://www.nltk.org/data.html <br />\n",
    "\n",
    "The following example demonstrates how to perform stemming and remove stop words from a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is from scikilt-learn module used for machine learning\n",
    "# http://scikit-learn.org/stable/\n",
    "import sklearn.feature_extraction.text as ext\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# pick the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# a function gets tokents and the stemmer algorithm and retuns all the stems as a list\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# a function that gets the text and use word_tokenize to create tokens\n",
    "# then calls function stem-tokens to get the stems\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "# the test used for training\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "\n",
    "# create CountVectorizer using tokenize function we defined and stop words for english. You can use stop words for \n",
    "# other languages too.\n",
    "vect = ext.CountVectorizer(tokenizer=tokenize, \n",
    "                           stop_words='english')\n",
    "\n",
    "# apply the CountVectorizer to the vocab\n",
    "vec = vect.fit(vocab)\n",
    "\n",
    "# apply the vec to new text and see how many stems are matching the training text\n",
    "sentence1 = vec.transform(['George loves swimming too!'])\n",
    "\n",
    "# prints the stem of training\n",
    "print vec.get_feature_names()\n",
    "# prints the number of training stems match the test text\n",
    "print sentence1.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing regular expressions\n",
    "\n",
    "Regular expressions present the data scientist with an interesting array of tools for parsing raw text. At first, it may seem daunting how regular expressions work. However, sites such as https://regex101.com/#python let you play with regular expressions so that you can see how the use of various expressions perform specific types of pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern-Matching Characters Used in Python\n",
    "\n",
    "(re) &nbsp;&nbsp;&nbsp; matching re <br />\n",
    "re?  &nbsp;&nbsp;&nbsp; matching 0 or 1 occurance <br />\n",
    "re*  &nbsp;&nbsp;&nbsp; matching 0 or more occurance <br />\n",
    "re+  &nbsp;&nbsp;&nbsp; matching 1 or more occurance <br />\n",
    ". &nbsp;&nbsp;&nbsp; matching any single character <br />\n",
    "[^...] &nbsp;&nbsp; matching any single character or range not in in brackets<br />\n",
    "[...] &nbsp;&nbsp;  matching any single character in brackets <br />\n",
    "re{n, m} &nbsp;&nbsp;&nbsp; matching at least n and at most m occurances<br />\n",
    "\\d &nbsp;&nbsp;&nbsp; matching a digit<br />\n",
    "\\s &nbsp;&nbsp;&nbsp; matching a whitespace (\\t\\n\\r\\f)<br />\n",
    "\\w &nbsp;&nbsp;&nbsp; matching word character<br />\n",
    "\\Z &nbsp;&nbsp;&nbsp; matching the end of a string (a\\Z means a at the end)<br />\n",
    "\\D &nbsp;&nbsp;&nbsp; matching nondigit<br />\n",
    "\\S &nbsp;&nbsp;&nbsp; matching nonwhitespace<br />\n",
    "\\W &nbsp;&nbsp;&nbsp; matching nonword characters<br />\n",
    "re1|re2 &nbsp;&nbsp;&nbsp; matching re1 or re2<br />\n",
    "re {n} &nbsp;&nbsp;&nbsp; matching exactly n times<br />\n",
    "re{n, } &nbsp;&nbsp;&nbsp; matching minimum n times<br />\n",
    "$ &nbsp;&nbsp;&nbsp; matching the end of the line<br />\n",
    "\n",
    "\n",
    "(?=foo)&nbsp;&nbsp;&nbsp;Lookahead&nbsp;&nbsp;&nbsp;Asserts that what immediately follows the current position in the string is foo <br />\n",
    "(?<=foo)&nbsp;&nbsp;&nbsp;Lookbehind&nbsp;&nbsp;&nbsp;Asserts that what immediately precedes the current position in the string is foo<br />\n",
    "(?!foo)&nbsp;&nbsp;&nbsp;Negative Lookahead&nbsp;&nbsp;&nbsp;Asserts that what immediately follows the current position in the string is not foo<br />\n",
    "(?<!foo)&nbsp;&nbsp;&nbsp;Negative Lookbehind&nbsp;&nbsp;&nbsp;Asserts that what immediately precedes the current position in the string is not  foo<br /><br />\n",
    "\n",
    "Example:<br />\n",
    "(?&lt;=&lt;hello&gt;).*(?=&lt;/hello&gt;)<br />\n",
    "returns test if being applied to &lt;hello&gt;test&lt;/hello&gt;<br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data1 = 'Welcome to data programming!'\n",
    "\n",
    "# compiles the regular expression so will be faster if you do it once (creates a DFA (Deterministic finite automata))\n",
    "# r means raw data so no need for escape sequences\n",
    "pattern = re.compile(r'a')\n",
    "\n",
    "# searches and find the first matching string\n",
    "dmatch2 = pattern.search(data1)\n",
    "\n",
    "# the matching string\n",
    "print dmatch2.group()\n",
    "# the start index of match\n",
    "print dmatch2.start()\n",
    "# the end index of match\n",
    "print dmatch2.end()\n",
    "\n",
    "# searches and find all the matching string\n",
    "dmatch1 = pattern.findall(data1)\n",
    "print dmatch1\n",
    "\n",
    "# find all matchings staring index and the matching string\n",
    "for match in pattern.finditer(data1):\n",
    "    print \"%s: %s\" % (match.start(), match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data1 = 'My phone number is: 800-555-1212.'\n",
    "data2 = '800-555-1234 is my phone number.'\n",
    "\n",
    "# compiles the regular expression so will be faster if you do it once (creates a DFA (Deterministic finite automata))\n",
    "pattern = re.compile(r'(\\d{3})-(\\d{3})-(\\d{4})')\n",
    "\n",
    "# searches and find the matching string\n",
    "dmatch1 = pattern.search(data1)\n",
    "dmatch2 = pattern.search(data2)\n",
    "\n",
    "# finds the match\n",
    "print dmatch1.group()\n",
    "print dmatch2.group()\n",
    "# finds the indices of start and end\n",
    "print dmatch1.start()\n",
    "print dmatch1.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the 20 newsgroups dataset\n",
    "\n",
    "The dataset is called “Twenty Newsgroups”. Here is the official description, quoted from the website:\n",
    "\n",
    "    The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the web-site and use the sklearn.datasets.load_files function by pointing it to the 20news-bydate-train subfolder of the uncompressed archive folder.\n",
    "\n",
    "In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the list of files matching those categories as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# used for training, and random_state is used for seeding the random shuffling\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# the type is a Bunch, a simple holder object with fields that can be both accessed as python dict keys or object\n",
    "print type(twenty_train)\n",
    "#twenty_train.data[0] or twenty_train['data']\n",
    "twenty_train['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shows the categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The files themselves are loaded in memory in the data attribute\n",
    "# Shows the number of files loaded\n",
    "# twenty_train.data is a list and each element is the whole file text\n",
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this can show you file names\n",
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# content of the first file\n",
    "# first three lines\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this gives you an integer assigned to each document and each integer represent one of the four categories we used\n",
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the category name for the first file loaded into memory\n",
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithms will require a category label for each document in the training set. In this case the category is the name of the newsgroup which also happens to be the name of the folder holding the individual documents.\n",
    "\n",
    "For speed and space efficiency reasons scikit-learn loads the target attribute as an array of integers that corresponds to the index of the category name in the target_names list. The category integer id of each sample is stored in the target attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get back the category names\n",
    "for i in twenty_train.target[:10]:\n",
    "    print twenty_train.target_names[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix \n",
    "\n",
    "A sparse matrix is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix.<br /><br />\n",
    "\n",
    "One of these algorithms is CSR (Compressed Sparse Row) algorithm.<br /><br />\n",
    "\n",
    "Example:<br /><br />\n",
    "\n",
    "[[1, 2, 0],<br />\n",
    " [0, 0, 3],<br />\n",
    " [4, 0, 5]]<br /><br />\n",
    " \n",
    "We will use CSR to compress this parse matrix. We end up with three matrices:<br /><br />\n",
    "\n",
    "NZV = [1, 2, 3, 4, 5]<br />\n",
    "C   = [0, 1, 2, 0, 2] <br />\n",
    "RP  = [0, 2, 3, 5]<br /><br />\n",
    "\n",
    "NZV = None zero values<br />\n",
    "C = Column of each none zero values (one for each NZV)<br />\n",
    "RP = Row Pointer (This is for finding the rows of element)<br /><br />\n",
    "\n",
    "This is interpretation of RP:<br />\n",
    "[0, 2, 3, 5] --> the last number is the number of NZV (so it is 5)<br />\n",
    "Now we will have [0, 2, 3] (and we know we have NZV indices as [0, 1, 2, 3, 4]<br />\n",
    "Now elements of (0, 1) in same row, (2) in next row, (3, 4) in next row.<br /><br />\n",
    "\n",
    "Having all above information about NZV, C and RP you can have the complete sparce matrix.<br />\n",
    "\n",
    "Another example:\n",
    "\n",
    "[[1, 2, 0],<br />\n",
    " [0, 0, 0],<br />\n",
    " [4, 0, 5]]<br /><br />\n",
    "\n",
    "NZV = [1, 2, 4, 5]<br />\n",
    "C   = [0, 1, 0, 2] <br />\n",
    "RP  = [0, 3, 3, 4]<br /><br />\n",
    "\n",
    "RP: (0,1) first row, 3 being repeated so you ignore the first 3 (means all zeros when you see repeatition and you should ignore) (3,4) goes to third row<br /><br />\n",
    "\n",
    "http://www.bu.edu/pasi/files/2011/01/NathanBell1-10-1000.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/sparse.html\n",
    "\n",
    "Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "#A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "A = csr_matrix([[1, 2, 3], [0, 0, 0], [0, 0, 5]])\n",
    "# gives you NZV\n",
    "A.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gives you C matrix\n",
    "A.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gives you RP matrix\n",
    "A.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shows you the sparse matrix \n",
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our text data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(['Data programming is cool', 'If you and only you pass the course!'])\n",
    "# tokenizing the two texts, as you can see we have 11 tokens\n",
    "count_vect.get_feature_names()\n",
    "#X_train_counts.toarray()\n",
    "#type(X_train_counts)\n",
    "#X_train_counts.indices\n",
    "#0, 6, 5, 3, 2, 3, 7, 1, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_train_counts) # as you can see this is CSR matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the sparse matrix itself is\n",
    "X_train_counts.toarray()\n",
    "# each row is for each of those texts in the list and each column is showing\n",
    "# if any token exists in this text (11 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NZV of Sparse matrix above\n",
    "X_train_counts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gives you index of the vocabulary\n",
    "count_vect.vocabulary_.get(u'only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the concept we talked above and the generated matrix is bag of words concept\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import sklearn.feature_extraction.text as ext\n",
    "import pandas as pd\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "\n",
    "\n",
    "count_vect = ext.CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(\n",
    "    twenty_train.data)\n",
    "\n",
    "# huge array but being saved as CSR matrix\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:\n",
    "\n",
    "http://www.cs.ucr.edu/~eamonn/CE/Bayesian%20Classification%20withInsect_examples.pdf\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, since they have already been fit to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with n-grams\n",
    "\n",
    "An n-gram is a continuous sequence of items in the text you want to analyze. The items can be letters, words etc. The n in n-gram refer to size. An n-gram that has a size of one, for example, is a unigram. The example in this section uses a size of three, making a trigram. You use n-grams in a probabillistic manner to perform tasks such as predicting the next sequence in a series, which wouldnt seem very useful until you start thinking about applications such as search engines that try to predict the word you want to type based on previous letters you have supplied. However, the technique has all sorts of applications, such as in DNA sequencing and data compression. The following example shows how to create n-grams from the 20 Newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import sklearn.feature_extraction.text as ext\n",
    "\n",
    "categories = ['sci.space']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', \n",
    "        categories=categories, \n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "        shuffle=True, \n",
    "        random_state=42)\n",
    "\n",
    "# analyzer can be word, char or char_wb(characters with word boundry)\n",
    "# word boundry means it is seperated from other words for example by space. \n",
    "# ngram_range = (min of n, max of n)\n",
    "# max_features = x means the top x ngram you looking for \n",
    "count_chars = ext.CountVectorizer(analyzer='char_wb', \n",
    "        ngram_range=(3,3), \n",
    "        max_features=10).fit(twenty_train['data'])\n",
    "\n",
    "print type(twenty_train)\n",
    "\n",
    "# stop words being used\n",
    "count_words = ext.CountVectorizer(analyzer='word', \n",
    "        ngram_range=(2,2),\n",
    "        max_features=10,\n",
    "        stop_words='english').fit(twenty_train['data'])\n",
    "\n",
    "X = count_chars.transform(twenty_train.data)\n",
    "\n",
    "# the top 10 features\n",
    "print count_chars.get_feature_names()\n",
    "# rows are number of documents\n",
    "# columns are the top 10 features\n",
    "# values are frequency of those features for each document\n",
    "print X.toarray()\n",
    "# the top 10 features\n",
    "print count_words.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NetworkX basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the initial graph\n",
    "\n",
    "Creating adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.cycle_graph(10)\n",
    "A = nx.adjacency_matrix(G)\n",
    "\n",
    "print(A.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "nx.draw_networkx(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.add_edge(1,5)\n",
    "nx.draw_networkx(G)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
